{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the test part for NLTK\n",
    "\n",
    "text = 'For a simple motor at least the rated power, rated speed, the supply voltage and the maximum current should be given.'\n",
    "sents = nltk.sent_tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['For a simple motor at least the rated power, rated speed, the supply voltage and the maximum current should be given.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [nltk.word_tokenize(sentences) for sentences in sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['For',\n",
       "  'a',\n",
       "  'simple',\n",
       "  'motor',\n",
       "  'at',\n",
       "  'least',\n",
       "  'the',\n",
       "  'rated',\n",
       "  'power',\n",
       "  ',',\n",
       "  'rated',\n",
       "  'speed',\n",
       "  ',',\n",
       "  'the',\n",
       "  'supply',\n",
       "  'voltage',\n",
       "  'and',\n",
       "  'the',\n",
       "  'maximum',\n",
       "  'current',\n",
       "  'should',\n",
       "  'be',\n",
       "  'given',\n",
       "  '.']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [nltk.pos_tag(tokens) for tokens in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('For', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('simple', 'JJ'),\n",
       "  ('motor', 'NN'),\n",
       "  ('at', 'IN'),\n",
       "  ('least', 'JJS'),\n",
       "  ('the', 'DT'),\n",
       "  ('rated', 'JJ'),\n",
       "  ('power', 'NN'),\n",
       "  (',', ','),\n",
       "  ('rated', 'VBN'),\n",
       "  ('speed', 'NN'),\n",
       "  (',', ','),\n",
       "  ('the', 'DT'),\n",
       "  ('supply', 'NN'),\n",
       "  ('voltage', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('the', 'DT'),\n",
       "  ('maximum', 'JJ'),\n",
       "  ('current', 'JJ'),\n",
       "  ('should', 'MD'),\n",
       "  ('be', 'VB'),\n",
       "  ('given', 'VBN'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tries \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Named Entity Recognition--NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1) What you do to design a simple motor. (english )\n",
      "\n",
      "For a simple motor at least the rated power, rated speed, the supply\n",
      "voltage and the maximum current should be given.\n",
      "\n",
      "The rated torque is calculated from power and speed, Tn = Pn/ nn *\n",
      "30/PI.\n",
      "\n",
      "For a simple motor with good efficiency a radial flux permanent magnet\n",
      "synchronous machine (PMSM) will be chosen, but the specification may\n",
      "require a different machine type like axial flux PMSM, separately\n",
      "excited synchronous, asynchronous or reluctance machine.\n",
      "\n",
      "Depending on the application also an “inner rotor” or “outer rotor”\n",
      "topology has to be chosen in case of a radial flux PMSM.\n",
      "\n",
      "If the machine size is not defined it has to be calculated from the\n",
      "physical limitation factors.\n",
      "\n",
      "The bore diameter and axial length need to be chosen to meet the mean\n",
      "shear stress requirements. Depending on the rated power an approximate\n",
      "mean shear stress value can be looked up from literature e.g. sigma =\n",
      "12 to 300 kN/m2 for P= 0.01 to 160 MVA.\n",
      "\n",
      "e.g. length = Tn / (Diameter^2 * PI * sigma).\n",
      "\n",
      "Then the limitations of the power supply have to be checked.\n",
      "\n",
      "Assuming an efficiency of 90% and a power factor of 0.9, the\n",
      "electrical input power is calculated from the rated power, Pel =\n",
      "Pn/0.9/0.9. From the power the required current will be calculated\n",
      "depending on the number of machine phases and the voltage, In =\n",
      "Pel/m/Udc/sqrt(6).\n",
      "\n",
      "Then a beneficial winding configuration has to be defined:\n",
      "\n",
      "e.g. A high fundamental winding factor is beneficial for torque\n",
      "generation, and different teeth pole combinations lead to less torque\n",
      "ripple and NVH (Noise, Vibration, Harshness). The chosen number of\n",
      "pole pairs and winding parallel paths depend on the supply\n",
      "limitations.\n",
      "\n",
      "Depending on the chosen winding configuration (distributed or\n",
      "concentrated) a stator topology can to be chosen. The magnet material,\n",
      "bore diameter and number of poles will limit the available rotor\n",
      "topologies. Rare-earth magnets are expensive (~50€/kg) but have a high\n",
      "energy density, while ferrite magnets are cheaper (~10€/kg) and less\n",
      "effective much more ferrite magnet material has to be used to deliver\n",
      "the same power.\n",
      "\n",
      "If all the above topics have been resolved an initial workflow can be\n",
      "set up.\n",
      "\n",
      "From this point on the work can be done in SyMSpace.\n",
      "\n",
      "SyMSpace MotorBox provides various machine simulation components,\n",
      "materials and geometries, which can be used to set up a workflow.\n",
      "\n",
      "The PMSM workflow contains the selected material, geometries, winding,\n",
      "a finite element simulation and a postprocessing step, to analyse at\n",
      "least the rated operational point.\n",
      "\n",
      "The geometry parameters have then to be adjusted manually in order to\n",
      "create a feasible model.  If the initial model is finished, a geometry\n",
      "optimization can be performed.  In order to find an optimal set of\n",
      "geometry parameters the parameter variation range and the objectives\n",
      "have to be defined. For a simple motor the objectives may be:\n",
      "\n",
      "1. Maximum efficiency at rated power.\n",
      "2. Minimum costs\n",
      "3/4. Low torque ripple and cogging torque e.g. 1% of nominal torque.\n",
      "5. THD of voltage below e.g. 10%\n",
      "6. Current density below e.g. 12 A/mm2 or Current Density times Current Sheet below 100 A2/mm3\n",
      "7. Power factor higher than e.g. 0.8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = open('motor_design1.txt',encoding='utf-8')\n",
    "raw = f.read()     \n",
    "#print (type(raw))   #classs 'str'\n",
    "print (raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = raw\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer()\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the entity tree of the text\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[5:]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            namedEnt = nltk.ne_chunk(tagged, binary=True)\n",
    "            namedEnt.draw()\n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_content() # only launch it when you want to see the analysis results for every sentences. I still can't find a proper way to make a output with reasonalbe number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Entity Name   Entity Type\n",
      "0    SyMSpace  ORGANIZATION\n",
      "1    MotorBox  ORGANIZATION\n"
     ]
    }
   ],
   "source": [
    "def parse_document(document):\n",
    "   document = re.sub('\\n', ' ', document)\n",
    "   if isinstance(document, str):\n",
    "       document = document\n",
    "   else:\n",
    "       raise ValueError('Document is not string!')\n",
    "   document = document.strip()\n",
    "   sentences = nltk.sent_tokenize(document)\n",
    "   sentences = [sentence.strip() for sentence in sentences]\n",
    "   return sentences\n",
    "\n",
    "# sample document\n",
    "text = \"\"\"\n",
    "SyMSpace MotorBox provides various machine simulation components,\n",
    "materials and geometries, which can be used to set up a workflow.\n",
    "\"\"\"\n",
    "\n",
    "# tokenize sentences\n",
    "sentences = parse_document(text)\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "# tag sentences and use nltk's Named Entity Chunker\n",
    "tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "ne_chunked_sents = [nltk.ne_chunk(tagged) for tagged in tagged_sentences]\n",
    "# extract all named entities\n",
    "named_entities = []\n",
    "for ne_tagged_sentence in ne_chunked_sents:\n",
    "   for tagged_tree in ne_tagged_sentence:\n",
    "       # extract only chunks having NE labels\n",
    "       if hasattr(tagged_tree, 'label'):\n",
    "           entity_name = ' '.join(c[0] for c in tagged_tree.leaves()) #get NE name\n",
    "           entity_type = tagged_tree.label() # get NE category\n",
    "           named_entities.append((entity_name, entity_type))\n",
    "           # get unique named entities\n",
    "           named_entities = list(set(named_entities))\n",
    "\n",
    "# store named entities in a data frame\n",
    "entity_frame = pd.DataFrame(named_entities, columns=['Entity Name', 'Entity Type'])\n",
    "# display results\n",
    "print(entity_frame)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First problem remain to be solved\n",
    "**We must find way to detect those entity in the mortor design process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"\"\"For a simple motor with good efficiency a radial flux permanent magnet\n",
    "synchronous machine (PMSM) will be chosen, but the specification may\n",
    "require a different machine type like axial flux PMSM, separately\n",
    "excited synchronous, asynchronous or reluctance machine.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = TextBlob(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['simple motor', 'good efficiency', 'radial flux', 'permanent magnet synchronous machine', 'pmsm', 'different machine type', 'axial flux', 'pmsm', 'reluctance machine']\n"
     ]
    }
   ],
   "source": [
    "print(blob.noun_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Analysis the meaning of sentences (the logic behind it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ORG: 'WHYY'] 'in' [LOC: 'Philadelphia']\n",
      "[ORG: 'McGlashan &AMP; Sarrail'] 'firm in' [LOC: 'San Mateo']\n",
      "[ORG: 'Freedom Forum'] 'in' [LOC: 'Arlington']\n",
      "[ORG: 'Brookings Institution'] ', the research group in' [LOC: 'Washington']\n",
      "[ORG: 'Idealab'] ', a self-described business incubator based in' [LOC: 'Los Angeles']\n",
      "[ORG: 'Open Text'] ', based in' [LOC: 'Waterloo']\n",
      "[ORG: 'WGBH'] 'in' [LOC: 'Boston']\n",
      "[ORG: 'Bastille Opera'] 'in' [LOC: 'Paris']\n",
      "[ORG: 'Omnicom'] 'in' [LOC: 'New York']\n",
      "[ORG: 'DDB Needham'] 'in' [LOC: 'New York']\n",
      "[ORG: 'Kaplan Thaler Group'] 'in' [LOC: 'New York']\n",
      "[ORG: 'BBDO South'] 'in' [LOC: 'Atlanta']\n",
      "[ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta']\n"
     ]
    }
   ],
   "source": [
    "IN = re.compile(r'.*\\bin\\b(?!\\b.+ing)')\n",
    "for doc in nltk.corpus.ieer.parsed_docs('NYT_19980315'):\n",
    "    for rel in nltk.sem.extract_rels('ORG', 'LOC', doc,\n",
    "        corpus='ieer', pattern = IN):\n",
    "        print(nltk.sem.rtuple(rel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I noticed that the big yellow dog(NN) is eating(V) fresh meat(N)\n"
     ]
    }
   ],
   "source": [
    "print('I noticed that the big yellow dog(NN) is eating(V) fresh meat(N)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Find Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['For', 'a', 'simple', 'motor', 'with', 'good', 'efficiency', 'a', 'radial', 'flux', 'permanent', 'magnet', 'synchronous', 'machine', '(', 'PMSM', ')', 'will', 'be', 'chosen', ',', 'but', 'the', 'specification', 'may', 'require', 'a', 'different', 'machine', 'type', 'like', 'axial', 'flux', 'PMSM', ',', 'separately', 'excited', 'synchronous', ',', 'asynchronous', 'or', 'reluctance', 'machine', '.']\n",
      "['For', 'simple', 'motor', 'good', 'efficiency', 'radial', 'flux', 'permanent', 'magnet', 'synchronous', 'machine', '(', 'PMSM', ')', 'chosen', ',', 'specification', 'may', 'require', 'different', 'machine', 'type', 'like', 'axial', 'flux', 'PMSM', ',', 'separately', 'excited', 'synchronous', ',', 'asynchronous', 'reluctance', 'machine', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "  \n",
    "example_sent = \"\"\"For a simple motor with good efficiency a radial flux permanent magnet\n",
    "synchronous machine (PMSM) will be chosen, but the specification may\n",
    "require a different machine type like axial flux PMSM, separately\n",
    "excited synchronous, asynchronous or reluctance machine.\"\"\"\n",
    "  \n",
    "stop_words = set(stopwords.words('english')) \n",
    "word_tokens = word_tokenize(example_sent) \n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "filtered_sentence = [] \n",
    "\n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        filtered_sentence.append(w) \n",
    "        \n",
    "print(word_tokens) \n",
    "print(filtered_sentence) \n",
    "\n",
    "result_sentence = [1]*len(filtered_sentence)\n",
    "for i in range(len(filtered_sentence)):\n",
    "    result_sentence[i] = filtered_sentence[i]\n",
    "    \n",
    "#print(result_sentence)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Find ambiguous words (Lesk algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "processedStopWords = set()\n",
    "for stopWord in stopWords:\n",
    "    tokens = nltk.word_tokenize(stopWord)\n",
    "    for token in tokens:\n",
    "        processedStopWords.add(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeskTokenize(s):\n",
    "    \"\"\"\n",
    "    分词工具\n",
    "    输入: 一个字符串s\n",
    "    输出: 一个有词构成的列表，不含stop word，数字和标点符号\n",
    "    \"\"\"\n",
    "    global processedStopWords\n",
    "    result = []\n",
    "    cleanString = ''\n",
    "    for c in s:\n",
    "        if c.isalpha() or c == ' ':\n",
    "            cleanString = cleanString + c\n",
    "    rawTokens = nltk.word_tokenize(cleanString)\n",
    "    for rawToken in rawTokens:\n",
    "        if rawToken not in processedStopWords:\n",
    "            result.append(rawToken)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap(a, b):\n",
    "    \"\"\"\n",
    "    此函数计算两个列表之间有多少相同的元素\n",
    "    输入: 两个列表a, b\n",
    "    输出: 两个列表中的相同元素个数，int\n",
    "    \"\"\"\n",
    "    result = 0\n",
    "    if len(a) == 0 or len(b) == 0:\n",
    "        return result\n",
    "    for x in a:\n",
    "        for y in b:\n",
    "            if x == y:\n",
    "                result += 1\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeskDisambiguisation(s, w):\n",
    "    \"\"\"\n",
    "    Lesk算法函数\n",
    "    输入: 1, 一个字符串(句子)s;\n",
    "         2, 一个字符串（需要消歧义的单词）w;\n",
    "    输出: 需要消歧义的词的在该句子中的含义;\n",
    "    \"\"\"\n",
    "    tokens = LeskTokenize(s)\n",
    "    # 特殊情况\n",
    "    if len(tokens) == 0:\n",
    "        raise Exception(\"Sorry, there is nothing in the sentence. \")\n",
    "    if w not in tokens:\n",
    "        raise Exception(\"Sorry, the word is not in the sentence or it is a stop word. \")\n",
    "    context = set() # 使用集合，避免重复\n",
    "    for token in tokens:\n",
    "        if token != w:\n",
    "            context.add(token)\n",
    "    if len(context) == 0:\n",
    "        raise Exception(\"Sorry, the word that needs desambiguation is the only word in the sentence. \")\n",
    "    contextDefinitions = []\n",
    "    for v in context:\n",
    "        for i in range(len(wn.synsets(v))):\n",
    "            definitionWords = LeskTokenize(wn.synset(wn.synsets(v)[i].name()).definition())\n",
    "            for definitionWord in definitionWords:\n",
    "                contextDefinitions.append(definitionWord)\n",
    "    maxValue = 0\n",
    "    maxNumber = 0\n",
    "    for j in range(len(wn.synsets(w))):\n",
    "        definitionWords = LeskTokenize(wn.synset(wn.synsets(w)[j].name()).definition())\n",
    "        currentValue = overlap(definitionWords, contextDefinitions)\n",
    "        if currentValue > maxValue:\n",
    "            maxValue = currentValue\n",
    "            maxNumber = j\n",
    "    return wn.synset(wn.synsets(w)[maxNumber].name()).definition()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a steady flow of a fluid (usually from natural causes)'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sent1 = \"\"\"For a simple motor at least the rated power, rated speed, the supply\n",
    "voltage and the maximum current should be given.\"\"\"\n",
    "\n",
    "example_sent2 = \"\"\"I went to the bank to deposit my money\"\"\"\n",
    "\n",
    "example_sent3 = \"\"\"River bank destroyed by flood\"\"\"\n",
    "\n",
    "LeskDisambiguisation(example_sent1,'current')\n",
    "#LeskDisambiguisation(example_sent2,'bank')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Identify the Verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['be', 'chosen', 'require', 'excited']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "lines = 'For a simple motor with good efficiency a radial flux permanent magnet synchronous machine (PMSM) will be chosen, but the specification may require a different machine type like axial flux PMSM, separately excited synchronous, asynchronous or reluctance machine..'\n",
    "# function to test if something is a noun\n",
    "is_noun = lambda pos: pos[:2] == 'VB'\n",
    "# do the nlp stuff\n",
    "tokenized = nltk.word_tokenize(lines)\n",
    "nouns = [word for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos)] \n",
    "\n",
    "print(nouns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Trying to extrat knowledge （remains problems）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['simple motor', 'rat power', 'rat', 'speed', 'supply voltage', 'be give']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from IPython.display import display\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "#word tokenizeing and part-of-speech tagger\n",
    "document = 'For a simple motor at least the rated power, rated speed, the supply voltage and the maximum current should be given.'\n",
    "tokens = [nltk.word_tokenize(sent) for sent in [document]]\n",
    "postag = [nltk.pos_tag(sent) for sent in tokens][0]\n",
    "\n",
    "# Rule for NP chunk and VB Chunk\n",
    "grammar = r\"\"\"\n",
    "    NBAR:\n",
    "        {<NN.*|JJ>*<NN.*>}  # Nouns and Adjectives, terminated with Nouns\n",
    "        {<RB.?>*<VB.?>*<JJ>*<VB.?>+<VB>?} # Verbs and Verb Phrases\n",
    "        \n",
    "    NP:\n",
    "        {<NBAR>}\n",
    "        {<NBAR><IN><NBAR>}  # Above, connected with in/of/etc...\n",
    "        \n",
    "\"\"\"\n",
    "#Chunking\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "\n",
    "# the result is a tree\n",
    "tree = cp.parse(postag)\n",
    "\n",
    "def leaves(tree):\n",
    "    \"\"\"Finds NP (nounphrase) leaf nodes of a chunk tree.\"\"\"\n",
    "    for subtree in tree.subtrees(filter = lambda t: t.label() =='NP'):\n",
    "        yield subtree.leaves()\n",
    "        \n",
    "def get_word_postag(word):\n",
    "    if pos_tag([word])[0][1].startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    if pos_tag([word])[0][1].startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    if pos_tag([word])[0][1].startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "def normalise(word):\n",
    "    \"\"\"Normalises words to lowercase and stems and lemmatizes it.\"\"\"\n",
    "    word = word.lower()\n",
    "    postag = get_word_postag(word)\n",
    "    word = lemmatizer.lemmatize(word,postag)\n",
    "    return word\n",
    "\n",
    "def get_terms(tree):    \n",
    "    for leaf in leaves(tree):\n",
    "        terms = [normalise(w) for w,t in leaf]\n",
    "        yield terms\n",
    "\n",
    "terms = get_terms(tree)\n",
    "\n",
    "features = []\n",
    "for term in terms:\n",
    "    _term = ''\n",
    "    for word in term:\n",
    "        _term += ' ' + word\n",
    "    features.append(_term.strip())\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Text summarization （remains problems）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
